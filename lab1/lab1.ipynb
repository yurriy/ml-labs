{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 - Градиентый спуск и линейные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть есть обучающая выборка $\\{x_i\\}_{i=1}^{\\mathcal{l}} \\subset \\mathbb{R}^{n}$, при этом каждому объекту в соответсвие поставлена метка класса $y_{i} \\in \\{-1, +1\\}$. Мы предполагаем, что в пространтсве $\\mathbb{R}^{n}$ существует гиперплоскость, которая относительно некоторой метрики \"хорошо\" разделяет объекты на два класса. При этом гиперплоскость задается параметрически:\n",
    "\n",
    "<center>\n",
    "$wx + b = 0$\n",
    "</center>\n",
    "\n",
    "Объект $x$ имеет метку $y = +1$, если $wx + b \\geq 0$ и $y = -1$ в ином случае. Вектор $w$ является нормалью к гиперплоскости, указывающий с какой стороны находятся объекты класса $y = +1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск модели ограничен до одного семейства, заданного параметрически. Обучение в таком случае сводится к задаче оптимизации эмпирического риска\n",
    "\n",
    "<center>\n",
    "$\\arg \\min_{\\theta} Q(\\theta) = \\arg \\min_{\\theta} \\frac{1}{l}\\sum_{i=1}^{\\mathcal{l}} \\mathcal{L}(a(x_i|\\theta), y_i)$, где\n",
    "</center>\n",
    "\n",
    "* $a(x|\\theta)$ - алгоритм из некотрого семейства, заданный параметром $\\theta$\n",
    "* $\\theta$ - вектор пространства параметров\n",
    "* $\\mathcal{L}$ - функция потерь, которая показывает на сколько точно предсказание\n",
    "\n",
    "Очевидно, что качество предсказания зависит от выбранной модели. Но также оно зависит и от выбора функции потерь $\\mathcal{L}$, которая существенно влияет на процесс обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В литературе можно встретить такое понятие, как отступ\n",
    "<center>$ M(x, y) = y\\cdot(wx + b)$,</center>\n",
    "его можно трактовать, как уровень удаление от гиперплоскости в сторону своего класса. Это позволит нам кратко записывать функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее естественной функцией потерь для задачи классификации является относительное количество неправильных классификаций, то есть\n",
    "<center>$ \\mathcal{L}(y_{pred}, y_{true}) = [y_{pred} \\neq y_{true}] = [M(x, y_{true}) < 0]$</center>\n",
    "\n",
    "Решение такой задачи является очень трудоемким, поэтому на практике производят оптимизацию реклаксированной ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К примеру **квадратичная ошибка**\n",
    "\n",
    "<center>$ Q(w) = \\frac{1}{\\mathcal{l}} \\sum_{i=1}^{\\mathcal{l}}((wx_i+b) - y_i)^{2}$</center>\n",
    "\n",
    "Она многим хороша, к примеру, в задачи оптимизации все сводится к выпуклому функционалу с локальным минимумом. Если представить, что признаки объекта $x_i$ записаны в матрицу $X$ построчно, а все метки записаны в вектор-столбец $Y$, то задача выглядит\n",
    "\n",
    "<center>\n",
    "$\\arg\\min_{w}||Xw - Y ||_{2}$,\n",
    "</center>\n",
    "\n",
    "и имеет аналитическое решение\n",
    "\n",
    "<center>\n",
    "$w = (X^TX)^{-1}X^TY$.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Сгенерируйте на плоскости 2 облака точек. Они должны слегка пересекаться, а точки внутри распределены нормально.\n",
    "2. Обучите линейную модель, разделяющую два облака точек, использую формулу выше.\n",
    "3. Изобразите облака с помощью библиотеки matplotlib, воспользуйтесь функцией scatter, для удобства точки можно сделать прозрачными.\n",
    "4. Постройте полученнную разделяющую прямую.\n",
    "5. Оцените сложность алгоритма обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_cloud(center, count):\n",
    "    points = []\n",
    "    for i in range(count):\n",
    "        point = np.random.normal(0.0, 3.0, 2)\n",
    "        points.append(point + center)\n",
    "    return points\n",
    "\n",
    "\n",
    "def solve(cloud1, cloud2):\n",
    "    matrix = cloud1 + cloud2\n",
    "    n = len(cloud1)\n",
    "    # we add 1 to each row of the matrix for computing b=w[0] in line equation\n",
    "    matrix = [[1] + row.tolist() for row in matrix]\n",
    "    pseudo_inverse_matrix = np.linalg.pinv(matrix)\n",
    "    labels = [1 if i < n else -1 for i in range(2 * n)]\n",
    "    return np.dot(pseudo_inverse_matrix, labels)\n",
    "\n",
    "\n",
    "def draw_plot(cloud1, cloud2, w):\n",
    "    plt.scatter([p[0] for p in cloud1], [p[1] for p in cloud1], c='g')\n",
    "    plt.scatter([p[0] for p in cloud2], [p[1] for p in cloud2], c='r')\n",
    "    plt.plot([x for x in range(-7, 15)], [(-w[0] - x * w[1]) / w[2] for x in range(-7, 15)], 'k-')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    n = 250\n",
    "    cloud1 = generate_cloud(np.array([0, 0]), n)\n",
    "    cloud2 = generate_cloud(np.array([6, 6]), n)\n",
    "    w = solve(cloud1, cloud2)\n",
    "    draw_plot(cloud1, cloud2, w)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще популярна следующая релаксация\n",
    "<center>$Q(w) = \\frac{1}{\\mathcal{l}} \\sum_{i=1}^{\\mathcal{l}} max(0, 1 - y_i\\cdot(wx_i + b))$,</center>\n",
    "если хотите узнать об этом более подробно, то вам стоит почитать про svm (support vector machine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая функция же обладает вероятностным смыслом\n",
    "\n",
    "<center>$ Q(w) = \\frac{1}{\\mathcal{l}} \\sum_{i=1}^{\\mathcal{l}} \\ln(1 + \\exp(-y_i\\cdot(wx_i + b)))$</center>\n",
    "В частности данный функционал приводит нас к оптимальному байесовскому классификатору при некоторых допущениях о распределении признаков. Но это совершенно отдельная история."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Пусть $\\mathbb{P}\\{y=1|x\\} = \\sigma(wx+b)$, где $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. Покажите, что задача\n",
    "<center>$ \\arg\\min_{w, b} \\sum_{x, y} \\ln(1 + \\exp(-y(wx + b)))$</center>\n",
    "есть ничто иное, как максимазиция правдоподобия.\n",
    "2. Отобразите все функционалы качества в осях $M \\times Q$ для одного элемента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи оптимизации не всегда существует аналитическое решение, либо оно может быть очень сложным. В таком случае используют численные методы. Да, речь идет именно о градиентном спуске. Это итеративный алгоритм, который устроен следующим образом. Пусть есть $Q(x)$, которую необходимо оптимизировать и она дифференцируема. Тогда задачу\n",
    "\n",
    "<center>$ \\arg\\min_{x} Q(x)$</center>\n",
    "\n",
    "можно решить следующим образом\n",
    "\n",
    "<center>$ x^{k+1} = x^{k} - \\lambda \\cdot \\triangledown Q(x)$,</center>\n",
    "\n",
    "где $\\lambda$ - некоторый шаг градиентного спуска, а $k$ - номер этого шага.\n",
    "\n",
    "От выбора правильного $\\lambda$ сильно зависит процесс обучения. Если взять слишком большое значение, то алгоритм может не сойтись. Если слишком малое, то обучение будет длиться долго. Также существует распространенный прием, применяемый часто при обучении нейросетей, уменьшать значение $\\lambda$ в соответствии с некоторым расписанием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Предложите какую-нибудь квадратичную функцию с глобальным минимумом.\n",
    "2. Найдите минимум методом градиентного спуска.\n",
    "3. Отобразите на плоскости линии уровней функции, которую вы оптимизируете.\n",
    "4. Покажите, какую траекторию проходит алгоритм градиентного спуска.\n",
    "5. Как вы выбрали значение $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение $\\lambda$ было выбрано так, чтобы алгоритм сходился достаточно быстро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(point):\n",
    "    x, y = point[0], point[1]\n",
    "    return x ** 2 + x * y + y ** 2\n",
    "\n",
    "\n",
    "def gradient(point):\n",
    "    x, y = point[0], point[1]\n",
    "    return np.array([2 * x + y, 2 * y + x])\n",
    "\n",
    "\n",
    "def render(levels, path):\n",
    "    # creating array of function values\n",
    "    x = np.arange(-3, 3, 0.025)\n",
    "    y = np.arange(-3, 3, 0.025)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f([X, Y])\n",
    "    levels.sort()\n",
    "    plt.contour(X, Y, Z, levels)\n",
    "    # drawing algorithm's path\n",
    "    plt.plot([p[0] for p in path], [p[1] for p in path], 'k-')\n",
    "\n",
    "\n",
    "def gradient_descend(start, path, levels):\n",
    "    cur_point, prev_point = start, start + 1\n",
    "    step_rate, eps = 0.1, 1 / 1e6\n",
    "    while math.fabs(f(cur_point) - f(prev_point)) > eps:\n",
    "        prev_point = cur_point.copy()\n",
    "        cur_point -= gradient(cur_point) * step_rate\n",
    "        path.append(cur_point.copy())\n",
    "        levels.append(f(cur_point))\n",
    "    return cur_point\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_point = np.array([1.9, 1.5])\n",
    "    path, levels = [], []\n",
    "    gradient_descend(start_point, path, levels)\n",
    "    render(levels, path)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют функции, которые плохо даются градиентному спуску. К примеру, функция Розенброка\n",
    "\n",
    "<center>$f(x, y) = (1-x)^2 + 100(y-x^2)^2$.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Проделайте все то же самое для функции Розенброка.\n",
    "2. Какую проблему вы наблюдаете?\n",
    "3. Как ее можно решить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм попадает на сторону продолговатого оврага, градиент направлен перпендикулярно ему, а направление, в котором находится глобальный минимум, параллельно оврагу, поэтому алгоритм просто прыгает с одной сторону оврага на другую, медленно двигаясь вдоль него к минимуму. Решение: прыгать 2 раза по градиенту, далее вектор, соединяющий 1 и 3 точку, будет направлен вдоль оврага, так как мы прыгнули с одной сторону на другую и вернулись обратно, продвинувшись немного вдоль оврага по направлению к минимуму. Теперь, зная направление, в котором находится минимум, нужно взять его вместо градиента и сделать третий прыжок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPS = 1 / 1e10\n",
    "\n",
    "def f(point):\n",
    "    x, y = point[0], point[1]\n",
    "    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n",
    "\n",
    "\n",
    "def gradient(point):\n",
    "    x, y = point[0], point[1]\n",
    "    return np.array([-2 * (1 - x) - 400 * (y - x ** 2) * x, 200 * (y - x ** 2)])\n",
    "\n",
    "\n",
    "def render(f, bounds, levels, path):\n",
    "    # creating array of function values\n",
    "    delta = 0.025\n",
    "    x = np.arange(bounds['min_x'], bounds['max_x'], delta)\n",
    "    y = np.arange(bounds['min_y'], bounds['max_y'], delta)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f([X, Y])\n",
    "    levels.sort()\n",
    "    plt.contour(X, Y, Z, levels, colors='#89D2A9')\n",
    "    # drawing algorithm's path\n",
    "    plt.plot([p[0] for p in path], [p[1] for p in path], 'k-')\n",
    "\n",
    "\n",
    "def make_step(f, cur_point, direction):\n",
    "    step_length = 1e5\n",
    "    while f(cur_point) < f(cur_point - direction * step_length):\n",
    "        step_length /= 2.99\n",
    "    cur_point -= direction * step_length\n",
    "    return cur_point\n",
    "\n",
    "\n",
    "def gradient_descend(f, start, path, levels):\n",
    "    path.append(start.copy())\n",
    "    levels.append(f(start))\n",
    "    prev_value = 1e18\n",
    "    cur_point = start\n",
    "\n",
    "    while math.fabs(f(cur_point) - prev_value) > EPS:\n",
    "        prev_value = f(cur_point)\n",
    "        point1 = cur_point.copy()\n",
    "        cur_point = make_step(f, cur_point, gradient(f, cur_point))\n",
    "        cur_point = make_step(f, cur_point, gradient(f, cur_point))\n",
    "        cur_point = make_step(f, cur_point, (point1 - cur_point))\n",
    "        path.append(cur_point.copy())\n",
    "        levels.append(f(cur_point))\n",
    "\n",
    "    return cur_point\n",
    "\n",
    "\n",
    "start_point = np.array([8.9, 13.5])\n",
    "bounds = {'min_x': -5, 'max_x': 2, 'min_y': -1, 'max_y': 16}\n",
    "path, levels = [], []\n",
    "print(gradient_descend(f, start_point, path, levels))\n",
    "render(f, bounds, levels, path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют различные модификации алгоритма градиентного спуска. К примеру, метод наискорейшего спуска, где значение $\\lambda$ зависит от шага\n",
    "\n",
    "<center>$\\lambda^{k} = \\arg\\min_{\\lambda}Q(x_k - \\lambda\\triangledown Q(x_k))$.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Снова разделите облака точек, только теперь оптимизируйте квадратичную ошибку метода градиентного спуска.\n",
    "2. Отобразите полученную прямую и облака точек.\n",
    "3. Сравните ответ с точным решением.\n",
    "4. Попробуйте метод наискорейшего спуска.\n",
    "5. Постройте график в осях (номер шага и значение $Q$).\n",
    "6. Сравните скорость сходимости обычного и наискорейшего спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_cloud(center, count):\n",
    "    points = []\n",
    "    for i in range(count):\n",
    "        point = np.random.normal(0.0, 3.0, 2)\n",
    "        points.append(point + center)\n",
    "    return points\n",
    "\n",
    "\n",
    "def gradient_descend(f, gradient, start_point):\n",
    "    cur_point, prev_point = start_point, start_point + 1\n",
    "    step_rate, eps = 1 / 1e5, 1 / 1e6\n",
    "    while math.fabs(f(cur_point) - f(prev_point)) > eps:\n",
    "        prev_point = cur_point.copy()\n",
    "        cur_point -= gradient(cur_point) * step_rate\n",
    "    return cur_point\n",
    "\n",
    "\n",
    "def get_strict_solution(matrix, labels):\n",
    "    pseudo_inverse_matrix = np.linalg.pinv(matrix)\n",
    "    return np.dot(pseudo_inverse_matrix, labels)\n",
    "\n",
    "\n",
    "def loss_f_gradient(w, x, y):\n",
    "    grad = [0, 0, 0]\n",
    "    for i in range(len(x)):\n",
    "        dot_product = np.dot(x[i], w)\n",
    "        grad[0] += 2 * (dot_product - y[i])\n",
    "        grad[1] += 2 * (dot_product - y[i]) * x[i][1]\n",
    "        grad[2] += 2 * (dot_product - y[i]) * x[i][2]\n",
    "    return np.array(grad)\n",
    "\n",
    "\n",
    "def loss_f(w, x, y):\n",
    "    ans = 0\n",
    "    for i in range(len(x)):\n",
    "        ans += (np.dot(w, x[i]) - y[i]) ** 2\n",
    "    return ans\n",
    "\n",
    "\n",
    "def solve(matrix, labels):\n",
    "    start_point = np.random.uniform(2, 10, 3)\n",
    "    solution = gradient_descend(\n",
    "        lambda point: loss_f(point, matrix, labels),\n",
    "        lambda point: loss_f_gradient(point, matrix, labels),\n",
    "        start_point)\n",
    "    return solution / np.linalg.norm(solution)\n",
    "\n",
    "\n",
    "def draw_plot(cloud1, cloud2, w1, w2):\n",
    "    plt.scatter([p[0] for p in cloud1], [p[1] for p in cloud1], c='g')\n",
    "    plt.scatter([p[0] for p in cloud2], [p[1] for p in cloud2], c='r')\n",
    "    plt.plot([x for x in range(-10, 20)], [(-w1[0] - x * w1[1]) / w1[2] for x in range(-10, 20)], 'b')\n",
    "    plt.plot([x for x in range(-10, 20)], [(-w2[0] - x * w2[1]) / w2[2] for x in range(-10, 20)], 'r')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    n = 250\n",
    "    cloud1 = generate_cloud(np.array([0, 0]), n)\n",
    "    cloud2 = generate_cloud(np.array([6, 6]), n)\n",
    "    matrix = cloud1 + cloud2\n",
    "    # we add 1 to each row of the matrix for computing b=w[0] in line equation\n",
    "    matrix = [[1] + row.tolist() for row in matrix]\n",
    "    labels = [1 if i < n else -1 for i in range(2 * n)]\n",
    "    time_start = time.clock()\n",
    "    w = solve(matrix, labels)\n",
    "    print(\"Gradient descends time: \", time.clock() - time_start)\n",
    "    w2 = get_strict_solution(matrix, labels)\n",
    "    w /= np.linalg.norm(w)\n",
    "    w2 /= np.linalg.norm(w2)\n",
    "    print('Accurate solution: ', w2)\n",
    "    print('Gradient descends solution: ', w)\n",
    "    print('Difference: ', w2 - w)\n",
    "    draw_plot(cloud1, cloud2, w, w2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И еще немного о проблемах градиентного спуска. Если у нас есть какие-то признаки, которые встречаются достаточно редко, то соответствующий столбец будет разряженным.\n",
    "\n",
    "**Задание**\n",
    "В чем заключается проблема?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если признак с номером $j$ встречается редко, то чаще всего градиент $g$ будет таким, что $g_j=0$, и $w_j$ почти не будет меняться, хотя для классификации некоторых элементов этот признак может быть критичным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также нужно понимать, что градиентный спуск может попасть в \"ловушку\" локального минимума. Обычно это актуально для нейросетей. Самый простой способо решить эту проблема - сдедать несколько запусков алгоритма или иметь какой-то инсайд, из какой точки стоит начинать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда количество данных может быть так велико, что даже градиентный спуск начинает работать медленно. Или же данные просто поступают к нам большим потоком, а параметры модели постепенно меняются. Тогда на сцену выходит метод стохастического градиента.\n",
    "\n",
    "Идея пределельно проста. Можно делать шаг спуска, вычисляя ошибку и градиент не для всех элементов выборки, а для какого-то небольшого количества или даже для одного объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Скачайте данные mnist c [Kaggle](https://www.kaggle.com/c/digit-recognizer).\n",
    "2. Обучите линейный классификатор 0 и 1, используйте логистическую функцию потерь.\n",
    "3. Проверьте качество классификации на отложенной выборке.\n",
    "<center>$ \\mathcal{L}(y_{pred}, y_{true}) = [y_{pred} \\neq y_{true}]$ </center>\n",
    "4. Как влияет размер батча на скорость и качество обучения?\n",
    "5. Отобразите графики, которые доказывает ваши слова (оси придумайте сами).\n",
    "6. Сколько проходов по данным вы делаете? Почему?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9988662131519275\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_data():\n",
    "    f = open('train.csv', 'r')\n",
    "    s = ' '\n",
    "    m, k = 0, 0\n",
    "    labels, matrix = [], []\n",
    "    while s:\n",
    "        s = f.readline()\n",
    "        if len(s) > 0 and (s[0] == '0' or s[0] == '1'):\n",
    "            matrix.append(list(map(int, s[2:].split(','))))\n",
    "            labels.append(int(s[0]))\n",
    "        k += 1\n",
    "    f.close()\n",
    "    f = open('train.txt', 'w')\n",
    "    for i in range(int(len(matrix) * 0.8)):\n",
    "        f.write(str(labels[i]) + ',' + ','.join(map(str, matrix[i])) + '\\n')\n",
    "    f.close()\n",
    "    f = open('check.txt', 'w')\n",
    "    for i in range(int(len(matrix) * 0.8), len(matrix)):\n",
    "        f.write(str(labels[i]) + ',' + ','.join(map(str, matrix[i])) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def read_data(file_name):\n",
    "    labels, matrix = [], []\n",
    "    f = open(file_name, 'r')\n",
    "    s = f.readline()\n",
    "    while s:\n",
    "        matrix.append(list(map(int, s[2:].split(','))))\n",
    "        labels.append(int(s[0]))\n",
    "        s = f.readline()\n",
    "    f.close()\n",
    "    matrix = [[1] + row for row in matrix]\n",
    "    labels = list(map(lambda x: x if x == 1 else -1, labels))\n",
    "    return {'matrix': matrix, 'labels': labels}\n",
    "\n",
    "\n",
    "def loss_f(w, x, y):\n",
    "    ans = 0.\n",
    "    for i in range(len(x)):\n",
    "        ans += math.log(1 + math.exp(-y[i] * np.dot(x[i], w)))\n",
    "    return ans\n",
    "\n",
    "\n",
    "def loss_f_gradient(w, x, y):\n",
    "    grad = [0] * len(w)\n",
    "    for i in range(len(x)):\n",
    "        e = math.exp(-y[i] * np.dot(x[i], w))\n",
    "        for j in range(len(w)):\n",
    "            grad[j] += -y[i] * x[i][j] * e / (1 + e)\n",
    "    return np.array(grad)\n",
    "\n",
    "\n",
    "def gradient_descend(f, gradient, start_point, matrix, labels):\n",
    "    eps, step_rate = 1 / 1e8, 1 / 1e6\n",
    "    cur_point, prev_point = start_point.copy(), start_point.copy()\n",
    "    prev_point += 0.1\n",
    "    cur, size = 0, 3\n",
    "    batch, batch_labels = matrix[cur:cur + size], labels[cur:cur + size]\n",
    "\n",
    "    while math.fabs(np.linalg.norm(cur_point - prev_point)) > eps:\n",
    "        prev_point = cur_point.copy()\n",
    "        cur_point -= gradient(cur_point, batch, batch_labels) * step_rate\n",
    "        cur += size\n",
    "        if cur > len(labels):\n",
    "            cur = 0\n",
    "        batch, batch_labels = matrix[cur:cur + size], labels[cur:cur + size]\n",
    "\n",
    "    return cur_point\n",
    "\n",
    "\n",
    "def solve(matrix, labels):\n",
    "    start_point = np.array([0.] * len(matrix[0]))\n",
    "    w = gradient_descend(loss_f, loss_f_gradient, start_point, matrix, labels)\n",
    "    return w\n",
    "\n",
    "\n",
    "def measure_quality(w, matrix, labels):\n",
    "    cnt = 0\n",
    "    for i in range(len(matrix)):\n",
    "        if labels[i] * np.dot(w, matrix[i]) >= 0:\n",
    "            cnt += 1\n",
    "    return cnt / len(matrix)\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = read_data('train.txt')\n",
    "    test_data = read_data('check.txt')\n",
    "    w = solve(data['matrix'], data['labels'])\n",
    "    print('Accuracy: ', measure_quality(w, test_data['matrix'], test_data['labels']))\n",
    "\n",
    "\n",
    "# parse_data()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='graph.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При больших размерах батча точность быстрее приближалась к 99.8% (на более ранних шагах градиентного спуска), но после завершения была не ниже 99.5 при любых размерах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У стохастического градиентного спуска также есть много всяких усовершествований, которые часто используются в реальной практике при обучении нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, текущее значение $Q$ можно вычислять с помощью экспоненциального сглаживания.\n",
    "<center>$Q^{k+1} = \\gamma Q^k + (1 - \\gamma) Q(x_{k+1})$, </center>\n",
    "\n",
    "где $Q(x_{k+1})$ вычисляется для обрабатываемого батча.\n",
    "\n",
    "**Задание**\n",
    "1. Как зависит график от $\\gamma$?\n",
    "2. Каким способом лучше вычислять $Q$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохранение импульса**\n",
    "\n",
    "Сохранения импульса позволяет избежать нам осциляции вдоль оси, по которой функция изменяется сильнее. Он заключается в том, что текущий градиентный шаг вычисляется на основе учета предыдущих шагов\n",
    "<center> $x^{k+1} = x^{k} - s^{k}$,</center> где $s^k = \\gamma s^{k-1} + \\lambda\\triangledown Q(x^k)$, при этом\n",
    " * $0 <\\gamma < 1$ - коэффициент учета предыдущего импульса\n",
    " * $s^{-1} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Найдите минимум $Q(x, y) = 10x^2 + y^2$ c помощью обычного метода.\n",
    "2. Воспользуйтесь методом сохранения импульса\n",
    "3. Отобразите и сравните треки.\n",
    "4. На основе чего вы выбрали $\\gamma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def f(point):\n",
    "    x, y = point[0], point[1]\n",
    "    return 10 * x ** 2 + y ** 2\n",
    "\n",
    "\n",
    "def gradient(point):\n",
    "    x, y = point[0], point[1]\n",
    "    return np.array([20 * x, 2 * y])\n",
    "\n",
    "\n",
    "def render(levels, path, path_color):\n",
    "    # creating array of function values\n",
    "    x = np.arange(-1.5, 1.5, 0.0025)\n",
    "    y = np.arange(-1, 2, 0.0025)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f([X, Y])\n",
    "    levels.sort()\n",
    "    plt.contour(X, Y, Z, levels)\n",
    "    # drawing algorithm's path\n",
    "    plt.plot([p[0] for p in path], [p[1] for p in path], path_color)\n",
    "\n",
    "\n",
    "def gradient_descend(start, path, levels):\n",
    "    cur_point, prev_point = start, start + 1\n",
    "    step_rate, eps = 0.1, 1 / 1e6\n",
    "    while math.fabs(f(cur_point) - f(prev_point)) > eps:\n",
    "        path.append(cur_point.copy())\n",
    "        levels.append(f(cur_point))\n",
    "        prev_point = cur_point.copy()\n",
    "        cur_point -= gradient(cur_point) * step_rate\n",
    "    return cur_point\n",
    "\n",
    "\n",
    "def gradient_descend_with_momentum(start, path, levels):\n",
    "    cur_point, prev_point = start, start + 1\n",
    "    step_rate, eps, l = 0.1, 1 / 1e6, 0.68\n",
    "    dx = np.array([0, 0])\n",
    "    while math.fabs(f(cur_point) - f(prev_point)) > eps:\n",
    "        path.append(cur_point.copy())\n",
    "        levels.append(f(cur_point))\n",
    "        prev_point = cur_point.copy()\n",
    "        dx = (1 - l) * dx + l * gradient(cur_point) * step_rate\n",
    "        cur_point -= dx\n",
    "    return cur_point\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_point = np.array([1.24, 1.35])\n",
    "    path, levels, path2, levels2 = [], [], [], []\n",
    "    gradient_descend(start_point.copy(), path, levels)\n",
    "    gradient_descend_with_momentum(start_point.copy(), path2, levels2)\n",
    "    render(levels, path, 'b')\n",
    "    render(levels2, path2, 'r')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Путь градиентного спуска с сохранением импульса колеблется намного меньше (так как если градиент резко меняет направление, то приращение уменьшается за счет градиентов предыдущих шагов), чем путь обычного, и совершает за счет этого немного меньше итераций. При слишком маленьком или слишком большом $\\lambda$ это преимущество пропадает, оптимальным оказалось $\\lambda\\approx 0.7$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ускоренный градиент Нестерова**\n",
    "\n",
    "И логическое развитие этого подхода приводит к методу ускоренного градиента Нестерова. Шаг спуска вычисляется немного иначе\n",
    "<center>$s^k = \\gamma s^{k-1} + \\lambda\\triangledown Q(x^k - s^{k-1})$,</center>\n",
    "то есть мы вычисляем градиент фукнции примерно в той точке, куда \"занесет\" нас накопленный импульс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Сравните этот метод и предыдущий на функции Розенброка.\n",
    "2. Отобразите и сравните треки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск с сохранением импульса ($\\approx 1500$ итераций)\n",
    "<img width=450 src='grad_momentum_Rozenbrock.jpg'>\n",
    "\n",
    "\n",
    "Ускоренный градиентный спуск Нестерова ($\\approx 1200$ итераций)\n",
    "\n",
    "\n",
    "<img width=500 src='grad_Nesterov_Rozenbrock.jpg'>\n",
    "\n",
    "\n",
    "Видно, что ускоренный градиент Нестерова позволяет лучше предсказать направление, в котором находится глобальный минимум, совершив меньше колебаний\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img scr='grad_momentum_Rozenbrock.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adagrad (2011)**\n",
    "\n",
    "Адаптивный градиент подразумевает вычисление $\\lambda$ для каждой размерности входного вектора $x$. Неформально говоря, для разряженных признаков он делает больший шаг, а для обычных шаг поменьше.\n",
    "<center> $x_{i}^{k + 1} = x_{i}^{k} - \\frac{\\lambda}{\\sqrt{G_{i, i}^k } + \\varepsilon} \\cdot \\frac{\\partial Q}{\\partial x_i}(x^k)$, где \n",
    "</center>\n",
    "\n",
    "\n",
    "* $G^{k} = \\sum_{t=1}^{k}g_t g_t^{T}$, где $g_t = \\triangledown Q(x^t)$.\n",
    "* $\\varepsilon$ - небольшая добавка, чтобы избежать деление на ноль.\n",
    "\n",
    "Как ни странно это улучшает сходимость процесса обучение, к примеру, при работе нейросетей с текстами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Обучите модель этим способом для mnist.\n",
    "2. Сравните сходимость с обычным стохастическим градиентным спуском (графики)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось сделать сходимость быстрее обычного только с потерей точности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSprop**\n",
    "\n",
    "Чтобы избежать постоянный рост знаменателя при $\\lambda$ можно воспользоваться следующим вариантом шага. Давайте будем подсчитывать матрицу $G^k$ только для какого-то небольшого количества последних шагов, это можно сделать к примеру с помощью экспоненциального сглаживания\n",
    "\n",
    "<center>$G^{k+1} = \\gamma G^{k} + (1 - \\gamma)g_{k+1}g_{k+1}^{T}$, где</center>\n",
    "$0< \\gamma < 1$ - чем больше значение, тем большее количество последних элементов мы учитываем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adadelta (2012)**\n",
    "\n",
    "**Задание**\n",
    "1. Изучите метод самостоятельно и кратко опишите.\n",
    "2. Как вы можете его объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы хотели оптимизировать градиентный спуск, и у них было 2 идеи.\n",
    "\n",
    "Первая состояла в том, что в случае Adagrad коэффициент при градиенте, $\\dfrac{1}{\\sqrt{\\sum^{t}||g||}}$, монотонно убывает, поэтому приращение стремится к нулю, и обучение со вмеренем прекращается. Они решили эту проблему тем, что суммируются длины не всех градиентов с начала, а $w$ последних. Но так как вычислять это неэффективно (нужно хранить в памяти $w$ последних градиентов), они поддерживают приближенное значение с помощью экспонециального сглаживания.\n",
    "\n",
    "Вторая идея заключалась в том, что приращение должно иметь ту же единицу измерения, что и $x$ (данное условие выполнено в методе Ньютона), и что для этого нужно домножать коэффециент при градиенте на $\\sqrt{\\sum \\limits_{k=t-w}^{t-1}||\\Delta x_k||}$. Коэффециент также вычисляется приближенно с помощью экспонециального сглаживания. Так как в числителе суммируются $\\Delta x_k, k=t-w,\\ldots, t-1$, то последнее приращение не влияет на числитель, но знаменатель увеличится, и это гарантирует, что если на текущем шаге градиент большой, то сильного изменения $x$ не произойдет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam (2015)**\n",
    "\n",
    "**Задание**\n",
    "1. Попробуйте скомбинировать метод сохранения импульса и RMSprop.\n",
    "2. Получили ли вы какое-то улучшение?\n",
    "3. Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Предложите некоторую функцию, которая наглядно показываеn отличие в работе всех предложенных методов.\n",
    "2. Сделайте анимацию, которая пошагово отрисовывает треки все спусков."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
